{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8704138,"sourceType":"datasetVersion","datasetId":5220723}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import models,layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import InceptionResNetV2","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:22:40.595620Z","iopub.execute_input":"2024-07-11T13:22:40.596030Z","iopub.status.idle":"2024-07-11T13:22:52.118765Z","shell.execute_reply.started":"2024-07-11T13:22:40.595998Z","shell.execute_reply":"2024-07-11T13:22:52.117766Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-11 13:22:42.266435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-11 13:22:42.266562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-11 13:22:42.377104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = '/kaggle/input/disasterdetection/data_disaster_types/train'\nval_data_dir = '/kaggle/input/disasterdetection/data_disaster_types/dev'\ntest_dir = '/kaggle/input/disasterdetection/data_disaster_types/test'","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:22:52.120744Z","iopub.execute_input":"2024-07-11T13:22:52.121488Z","iopub.status.idle":"2024-07-11T13:22:52.126916Z","shell.execute_reply.started":"2024-07-11T13:22:52.121453Z","shell.execute_reply":"2024-07-11T13:22:52.126071Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\n\n\n\n# Get the list of class folders\nclass_folders = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n\n# Iterate through each class folder and count the number of images\nfor class_folder in class_folders:\n    class_path = os.path.join(data_dir, class_folder)\n    num_images = len(os.listdir(class_path))\n    print(f\"Class: {class_folder}, Number of Images: {num_images}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:22:52.128035Z","iopub.execute_input":"2024-07-11T13:22:52.128383Z","iopub.status.idle":"2024-07-11T13:22:54.897862Z","shell.execute_reply.started":"2024-07-11T13:22:52.128353Z","shell.execute_reply":"2024-07-11T13:22:54.896886Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Class: flood, Number of Images: 2336\nClass: hurricane, Number of Images: 1444\nClass: other_disaster, Number of Images: 1132\nClass: fire, Number of Images: 1270\nClass: landslide, Number of Images: 940\nClass: earthquake, Number of Images: 2058\nClass: not_disaster, Number of Images: 3666\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define input and output directories\ninput_dir = '/kaggle/input/disasterdetection/data_disaster_types/train'\noutput_dir = '/kaggle/working/augmented_images'\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Copy images from input directory to output directory, excluding 'u.txt'\nfor class_folder in os.listdir(input_dir):\n    if class_folder != 'u.txt' and os.path.isdir(os.path.join(input_dir, class_folder)):\n        class_output_dir = os.path.join(output_dir, class_folder)\n        os.makedirs(class_output_dir, exist_ok=True)  # Create class output directory\n        os.makedirs(os.path.join(class_output_dir, 'subdirectory'), exist_ok=True)  # Create subdirectory\n        for filename in os.listdir(os.path.join(input_dir, class_folder)):\n            src = os.path.join(input_dir, class_folder, filename)\n            dst = os.path.join(class_output_dir, 'subdirectory', filename)\n            if os.path.isfile(src) and filename != 'u.txt':\n                shutil.copy(src, dst)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:22:54.899895Z","iopub.execute_input":"2024-07-11T13:22:54.900196Z","iopub.status.idle":"2024-07-11T13:24:58.527623Z","shell.execute_reply.started":"2024-07-11T13:22:54.900170Z","shell.execute_reply":"2024-07-11T13:24:58.526654Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n\n# Define output directory\noutput_dir = '/kaggle/working/augmented_images'\n\n# Find the maximum number of images among all class directories\nmax_images = 0\nfor class_folder in os.listdir(output_dir):\n    class_folder_path = os.path.join(output_dir, class_folder, 'subdirectory')\n    if os.path.isdir(class_folder_path):\n        num_images = len(os.listdir(class_folder_path))\n        max_images = max(max_images, num_images)\n\n# Perform data augmentation to match the maximum number of images in each class directory\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    \n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\n\nfor class_folder in os.listdir(output_dir):\n    class_folder_path = os.path.join(output_dir, class_folder, 'subdirectory')\n    if os.path.isdir(class_folder_path):\n        num_images = len(os.listdir(class_folder_path))\n        num_images_to_add = max_images - num_images\n        i = 0\n        if num_images_to_add > 0:\n            print(f\"Augmenting images for class {class_folder}...\")\n            for filename in os.listdir(class_folder_path):\n                \n                if filename.endswith(('.jpg', '.jpeg', '.png')):\n                    image_path = os.path.join(class_folder_path, filename)\n                    img = load_img(image_path)  # Load image using appropriate method\n                    x = img_to_array(img)  # Convert image to numpy array\n                    x = x.reshape((1,) + x.shape)  # Reshape to (1, height, width, channels) for flow() method\n                    j = 0\n                    for batch in datagen.flow(x, batch_size=1, save_to_dir=class_folder_path, save_prefix='augmented', save_format='jpeg'):\n                        j += 1\n                        i += 1\n                        if j == 10:  # Apply augmentation techniques until reaching the target number\n                            break\n                        if i == num_images_to_add:\n                            break\n                    if i == num_images_to_add:\n                        break  \n                    \n\nprint(\"Augmentation completed successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T13:24:58.528950Z","iopub.execute_input":"2024-07-11T13:24:58.529239Z","iopub.status.idle":"2024-07-11T14:03:38.179573Z","shell.execute_reply.started":"2024-07-11T13:24:58.529215Z","shell.execute_reply":"2024-07-11T14:03:38.178555Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Augmenting images for class fire...\nAugmenting images for class flood...\nAugmenting images for class hurricane...\nAugmenting images for class earthquake...\nAugmenting images for class other_disaster...\nAugmenting images for class landslide...\nAugmentation completed successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the parent directory\nparent_directory = \"/kaggle/working/augmented_images\"\n\n# List of subdirectories to process\nsubdirectories = ['earthquake','flood','landslide','other_disaster','not_disaster','fire','hurricane']\n\n# Loop through each subdirectory\nfor subdir in subdirectories:\n    subdirectory_path = os.path.join(parent_directory, subdir)\n\n    # List all files in the subdirectory\n    files = os.listdir(os.path.join(subdirectory_path, 'subdirectory'))\n\n    # Move each file to the parent directory\n    for file in files:\n        source = os.path.join(subdirectory_path, 'subdirectory', file)\n        destination = os.path.join(subdirectory_path, file)\n        shutil.move(source, destination)\n\n    # Remove the now empty subdirectory\n    os.rmdir(os.path.join(subdirectory_path, 'subdirectory'))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:38.180890Z","iopub.execute_input":"2024-07-11T14:03:38.181248Z","iopub.status.idle":"2024-07-11T14:03:38.989404Z","shell.execute_reply.started":"2024-07-11T14:03:38.181216Z","shell.execute_reply":"2024-07-11T14:03:38.988640Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for class_folder in class_folders:\n    num_images = len(os.listdir(f'/kaggle/working/augmented_images/{class_folder}'))\n    print(f\"Class: {class_folder}, Number of Images: {num_images}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:38.990537Z","iopub.execute_input":"2024-07-11T14:03:38.990841Z","iopub.status.idle":"2024-07-11T14:03:39.010924Z","shell.execute_reply.started":"2024-07-11T14:03:38.990815Z","shell.execute_reply":"2024-07-11T14:03:39.010053Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Class: flood, Number of Images: 3586\nClass: hurricane, Number of Images: 3435\nClass: other_disaster, Number of Images: 3350\nClass: fire, Number of Images: 3416\nClass: landslide, Number of Images: 3300\nClass: earthquake, Number of Images: 3540\nClass: not_disaster, Number of Images: 3666\n","output_type":"stream"}]},{"cell_type":"code","source":"data_dir = \"/kaggle/working/augmented_images\"","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:39.011895Z","iopub.execute_input":"2024-07-11T14:03:39.012160Z","iopub.status.idle":"2024-07-11T14:03:39.016184Z","shell.execute_reply.started":"2024-07-11T14:03:39.012136Z","shell.execute_reply":"2024-07-11T14:03:39.015249Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"Imagesize = 224\n\n\nseed = 123\n\ntraining_dataset= tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    image_size=(Imagesize,Imagesize),\n    batch_size=32,\n    shuffle=True,\n    seed=seed )\n\nvalidation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    val_data_dir,\n    image_size=(Imagesize,Imagesize),\n    batch_size=32,\n    shuffle=True,\n    seed=seed )\n\ntest_dataset  = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    image_size=(Imagesize,Imagesize),\n    batch_size=32,\n    shuffle=True,\n    seed=seed )","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:39.017214Z","iopub.execute_input":"2024-07-11T14:03:39.017500Z","iopub.status.idle":"2024-07-11T14:03:44.706693Z","shell.execute_reply.started":"2024-07-11T14:03:39.017476Z","shell.execute_reply":"2024-07-11T14:03:44.705952Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Found 24293 files belonging to 7 classes.\nFound 1470 files belonging to 7 classes.\nFound 3195 files belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"rescale = tf.keras.Sequential([\n    layers.Rescaling(1./255),\n])\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomContrast(0.1),  \n    layers.RandomFlip(mode='horizontal_and_vertical'),  \n    layers.RandomRotation(factor=0.2),  \n    \n])","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:44.709909Z","iopub.execute_input":"2024-07-11T14:03:44.710192Z","iopub.status.idle":"2024-07-11T14:03:44.731738Z","shell.execute_reply.started":"2024-07-11T14:03:44.710167Z","shell.execute_reply":"2024-07-11T14:03:44.731018Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nbase_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model2 = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\nbase_model3 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:44.732773Z","iopub.execute_input":"2024-07-11T14:03:44.733110Z","iopub.status.idle":"2024-07-11T14:03:53.826159Z","shell.execute_reply.started":"2024-07-11T14:03:44.733076Z","shell.execute_reply":"2024-07-11T14:03:53.825372Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model(base_model):\n    \n    model = models.Sequential()\n\n   \n    for layer in base_model.layers[:-5]:\n        layer.trainable = False\n\n    model = models.Sequential()\n\n   \n    model.add(rescale)\n    model.add(data_augmentation)\n\n    model.add(base_model)\n\n    \n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dense(512, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(7, activation='softmax'))  \n    input  =  (32,224,224,3)\n    model.build(input_shape = input)\n    \n    optimizer = Adam(learning_rate=0.0001)\n\n    model.compile(\n    optimizer=optimizer,\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n    )\n    \n    print(model.summary())\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:53.827319Z","iopub.execute_input":"2024-07-11T14:03:53.827631Z","iopub.status.idle":"2024-07-11T14:03:53.837203Z","shell.execute_reply.started":"2024-07-11T14:03:53.827606Z","shell.execute_reply":"2024-07-11T14:03:53.836174Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## InceptionResNetV2\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"model = create_model(base_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:53.838690Z","iopub.execute_input":"2024-07-11T14:03:53.839013Z","iopub.status.idle":"2024-07-11T14:03:54.323969Z","shell.execute_reply.started":"2024-07-11T14:03:53.838983Z","shell.execute_reply":"2024-07-11T14:03:54.323135Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ inception_resnet_v2             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1536\u001b[0m)       │    \u001b[38;5;34m54,336,736\u001b[0m │\n│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1536\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1536\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │     \u001b[38;5;34m1,573,888\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │     \u001b[38;5;34m1,049,600\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │        \u001b[38;5;34m65,664\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m7\u001b[0m)                │           \u001b[38;5;34m903\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ inception_resnet_v2             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,888</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,551,591\u001b[0m (219.54 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,551,591</span> (219.54 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,345,191\u001b[0m (28.02 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,345,191</span> (28.02 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m50,206,400\u001b[0m (191.52 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,206,400</span> (191.52 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',   \n    min_delta=0.001,          \n    patience=10,              \n    verbose=1,               \n    mode='max',             \n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:54.324935Z","iopub.execute_input":"2024-07-11T14:03:54.325216Z","iopub.status.idle":"2024-07-11T14:03:54.333276Z","shell.execute_reply.started":"2024-07-11T14:03:54.325191Z","shell.execute_reply":"2024-07-11T14:03:54.332372Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"record = model.fit(\n    training_dataset,\n    batch_size=32,\n    epochs=40,\n    verbose=1,\n    validation_data=validation_dataset,\n    shuffle=True,\n    callbacks=[early_stopping]      \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T14:03:54.334361Z","iopub.execute_input":"2024-07-11T14:03:54.334752Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 237ms/step - accuracy: 0.4107 - loss: 1.5551 - val_accuracy: 0.6395 - val_loss: 0.9975\nEpoch 2/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.6145 - loss: 1.0809 - val_accuracy: 0.6531 - val_loss: 0.9580\nEpoch 3/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.6579 - loss: 0.9706 - val_accuracy: 0.6503 - val_loss: 0.9796\nEpoch 4/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.6882 - loss: 0.8835 - val_accuracy: 0.6633 - val_loss: 0.9416\nEpoch 5/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 224ms/step - accuracy: 0.7080 - loss: 0.8464 - val_accuracy: 0.6816 - val_loss: 0.9282\nEpoch 6/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 224ms/step - accuracy: 0.7187 - loss: 0.8213 - val_accuracy: 0.6748 - val_loss: 0.9346\nEpoch 7/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 224ms/step - accuracy: 0.7350 - loss: 0.7671 - val_accuracy: 0.6789 - val_loss: 0.9100\nEpoch 8/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.7413 - loss: 0.7484 - val_accuracy: 0.6871 - val_loss: 0.9009\nEpoch 9/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.7535 - loss: 0.7107 - val_accuracy: 0.6837 - val_loss: 0.8922\nEpoch 10/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 224ms/step - accuracy: 0.7586 - loss: 0.6926 - val_accuracy: 0.6816 - val_loss: 0.8992\nEpoch 11/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 224ms/step - accuracy: 0.7726 - loss: 0.6635 - val_accuracy: 0.6980 - val_loss: 0.8766\nEpoch 12/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 224ms/step - accuracy: 0.7801 - loss: 0.6420 - val_accuracy: 0.7020 - val_loss: 0.8935\nEpoch 13/40\n\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 225ms/step - accuracy: 0.7872 - loss: 0.6232 - val_accuracy: 0.7068 - val_loss: 0.8451\nEpoch 14/40\n\u001b[1m 72/760\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 212ms/step - accuracy: 0.7902 - loss: 0.5756","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n\ntest_data = []\ntest_labels = []\nfor data, labels in test_dataset:\n    test_data.append(data.numpy())\n    test_labels.append(labels.numpy())\n\ntest_data = np.concatenate(test_data)\ntest_labels = np.concatenate(test_labels)\n\npredictions = model.predict(test_data)\npredicted_labels = np.argmax(predictions, axis=1)  # Assuming a classification problem with softmax output\n# If labels are one-hot encoded\n\n# Compute the confusion matrix\ncm = confusion_matrix(test_labels, predicted_labels)\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Visualize the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## InceptionV3","metadata":{}},{"cell_type":"code","source":"model2 = create_model(base_model2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record = model2.fit(\n    training_dataset,\n    batch_size=32,\n    epochs=25,\n    verbose=1,\n    validation_data=validation_dataset,\n    shuffle=True,\n    callbacks=[early_stopping]      \n\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VGG 16","metadata":{}},{"cell_type":"code","source":"model3 = create_model(base_model3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record = model3.fit(\n    training_dataset,\n    batch_size=32,\n    epochs=25,\n    verbose=1,\n    validation_data=validation_dataset,\n    shuffle=True,\n    callbacks=[early_stopping]      \n\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss, accuracy = model.evaluate(test_dataset)\n# print(f\"Test Loss: {loss}\")\n# print(f\"Test Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n\n\ntest_datagen = ImageDataGenerator(rescale=None)  \n\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(Imagesize, Imagesize),  \n    batch_size=32,\n    class_mode='categorical',  \n    shuffle=False  \n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_classes = test_generator.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_generator, verbose=1)\npredictions2 = model2.predict(test_generator, verbose=1)\npredictions3 = model3.predict(test_generator, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Soft voting","metadata":{}},{"cell_type":"code","source":"import numpy as np\naverage_prob = (predictions + predictions2 + predictions3) / 3\n\ny_pred_soft = np.argmax(average_prob, axis=1)\n\naccuracy_soft = np.mean(y_pred_soft == true_classes)\nprint('Accuracy:', accuracy_soft)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hard Voting","metadata":{}},{"cell_type":"code","source":"from scipy.stats import mode\n\npreds = np.argmax(predictions, axis=1)\npreds2 = np.argmax(predictions2, axis=1)\npreds3 = np.argmax(predictions3, axis=1)\nmax_voting = np.stack([preds, preds2, preds3], axis=1)\n\nmax_voting.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_hard, _ = mode(max_voting, axis=1)\ny_pred_hard = y_pred_hard.ravel()\n\naccuracy = np.mean(y_pred_hard == true_classes)\nprint('Accuracy:', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}